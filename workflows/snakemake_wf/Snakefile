"""
Snakemake workflow for tldr - Transposable Element Detection from Long Reads
Container-only execution with Singularity/Apptainer
Supports paired/multiple BAMs per patient (e.g., tumor/normal pairs)
"""

import pandas as pd
from pathlib import Path

# Load configuration
configfile: "config.yaml"

# Load samples from TSV file
samples_df = pd.read_csv(config["samples"], sep="\t")

# Group samples by patient to support paired analysis
# Each patient can have multiple samples (tumor/normal, timepoints, etc.)
PATIENTS = samples_df["patient"].unique().tolist()

def get_patient_bams(wildcards):
    """Get all BAM files for a patient"""
    patient_samples = samples_df[samples_df["patient"] == wildcards.patient]
    return patient_samples["path"].tolist()

def get_patient_bais(wildcards):
    """Get all BAI files for a patient"""
    patient_samples = samples_df[samples_df["patient"] == wildcards.patient]
    return [bam + ".bai" for bam in patient_samples["path"].tolist()]

def get_bam_list(wildcards):
    """Get comma-separated BAM list for tldr --bams parameter"""
    patient_samples = samples_df[samples_df["patient"] == wildcards.patient]
    bams = patient_samples["path"].tolist()
    return ",".join(bams)

# Output directory
OUTDIR = config.get("outdir", "results")

# Container image
TLDR_CONTAINER = config.get("container", "docker://sahuno/tldr:latest")

# ================================================================================
# Target Rules
# ================================================================================

rule all:
    input:
        expand(f"{OUTDIR}/{{patient}}/{{patient}}.table.txt", patient=PATIENTS),
        f"{OUTDIR}/summary/all_patients.merged.txt",
        f"{OUTDIR}/summary/summary_stats.txt"

# ================================================================================
# tldr Detection Rules
# ================================================================================

rule tldr_detect:
    """
    Run tldr transposable element detection on paired/multiple BAMs per patient
    Supports tumor/normal pairs, case/control, timepoints, etc.
    """
    input:
        bams = get_patient_bams,
        bais = get_patient_bais,
        ref = config["reference_genome"]
    output:
        table = f"{OUTDIR}/{{patient}}/{{patient}}.table.txt"
    params:
        te_ref = config.get("te_reference", "/opt/tldr/ref/teref.ont.human.fa"),
        nonref = lambda wildcards: f"-n {config['nonref_collection']}" if config.get('nonref_collection') else "",
        extra = config.get("tldr_params", ""),
        outbase = lambda wildcards: f"{OUTDIR}/{wildcards.patient}/{wildcards.patient}",
        bam_list = get_bam_list
    threads: config.get("threads", 4)
    resources:
        mem_mb_per_cpu = config.get("mem_mb_per_cpu", 8000),
        runtime = config.get("runtime", 240),
        slurm_partition = config.get("slurm_partition", "componc_cpu"),
        slurm_account = config.get("slurm_account", "greenbab"),
        nodes = 1
    container: TLDR_CONTAINER
    log:
        f"{OUTDIR}/logs/{{patient}}.tldr.log"
    shell:
        """
        tldr \
            --bams {params.bam_list} \
            -e {params.te_ref} \
            -r {input.ref} \
            -o {params.outbase} \
            -p {threads} \
            {params.nonref} \
            {params.extra} \
            --min_te_len 100 \
            --max_cluster_size 100 \
            --trdcol \
            --detail_output \
            --methylartist \
            --keep_pickles \
            --color_consensus \
            2>&1 | tee {log}
        """

rule index_bam:
    """
    Index BAM file if index doesn't exist
    """
    input:
        bam = "{file}.bam"
    output:
        bai = "{file}.bam.bai"
    threads: 1
    resources:
        mem_mb_per_cpu = 4000,
        runtime = 60,
        slurm_partition = "cpushort",
        slurm_account = config.get("slurm_account", "greenbab"),
        nodes = 1
    container: TLDR_CONTAINER
    shell:
        """
        samtools index {input.bam}
        """

# ================================================================================
# Summary and Reporting Rules
# ================================================================================

rule merge_results:
    """
    Merge all tldr results into a single table
    """
    input:
        tables = expand(f"{OUTDIR}/{{patient}}/{{patient}}.table.txt", patient=PATIENTS)
    output:
        merged = f"{OUTDIR}/summary/all_patients.merged.txt"
    params:
        patients = PATIENTS
    container: TLDR_CONTAINER
    run:
        import pandas as pd

        dfs = []
        for patient, table in zip(params.patients, input.tables):
            df = pd.read_csv(table, sep="\t")
            df.insert(0, "Patient", patient)
            dfs.append(df)

        merged_df = pd.concat(dfs, ignore_index=True)
        merged_df.to_csv(output.merged, sep="\t", index=False)

rule summary_stats:
    """
    Generate summary statistics across all patients
    """
    input:
        merged = f"{OUTDIR}/summary/all_patients.merged.txt"
    output:
        stats = f"{OUTDIR}/summary/summary_stats.txt",
        counts = f"{OUTDIR}/summary/insertion_counts_per_patient.txt"
    container: TLDR_CONTAINER
    run:
        import pandas as pd

        df = pd.read_csv(input.merged, sep="\t")

        # Summary statistics
        with open(output.stats, "w") as f:
            f.write("=== tldr Summary Statistics ===\n\n")
            f.write(f"Total patients: {df['Patient'].nunique()}\n")
            f.write(f"Total insertions detected: {len(df)}\n")
            f.write(f"Unique chromosomes: {df['Chrom'].nunique()}\n\n")

            f.write("Insertions by TE Family:\n")
            family_counts = df['Family'].value_counts()
            for family, count in family_counts.items():
                f.write(f"  {family}: {count}\n")

            f.write("\nInsertions by TE Subfamily:\n")
            subfamily_counts = df['Subfamily'].value_counts().head(10)
            for subfamily, count in subfamily_counts.items():
                f.write(f"  {subfamily}: {count}\n")

        # Per-patient insertion counts
        patient_counts = df.groupby('Patient').size().reset_index(name='InsertionCount')
        patient_counts.to_csv(output.counts, sep="\t", index=False)

# ================================================================================
# Utility Rules
# ================================================================================

rule clean:
    """
    Remove output directory
    """
    shell:
        f"rm -rf {OUTDIR}"
